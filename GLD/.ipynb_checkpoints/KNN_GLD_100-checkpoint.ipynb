{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from numpy import sqrt \n",
    "import time\n",
    "\n",
    "#Tien Xu Ly\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "#Draw Flot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Cacuale error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "#distance Libaray\n",
    "from dtw import *\n",
    "from scipy.spatial.distance import euclidean\n",
    "\n",
    "#FFNN Libarary\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from scikeras.wrappers import KerasRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Đọc Dữ Liệu\n",
    "# @param   filePath     Đường dẫn tập dữ liệu CSV\n",
    "# @return  df           Tập dữ liệu csv dưới dạng df\n",
    "def readData(filePath):\n",
    "    # Load dữ liệu\n",
    "    dataCSV = pd.read_csv(filePath)\n",
    "    df=dataCSV[['GLD']]\n",
    "    return df\n",
    "\n",
    "# Tiền Xử lý Dữ Liệu\n",
    "# @param  Data   Tập dữ liệu\n",
    "# @return df     Dữ liệu đã được tiền xử lý\n",
    "def cleanData(df):\n",
    "    # Replace null values with 0\n",
    "    df.fillna(0, inplace=True)\n",
    "    # Remove outliers by replacing values outside of 10 standard deviations with the mean\n",
    "    std = df['GLD'].std()\n",
    "    mean = df['GLD'].mean()\n",
    "    df['GLD'] = np.where(df['GLD'] > (mean + 10*std), mean, df['GLD'])\n",
    "    df['GLD'] = np.where(df['GLD'] < (mean - 10*std), mean, df['GLD'])\n",
    "    # Scale data_AMZN to range [0, 1]\n",
    "    scaler = MinMaxScaler()\n",
    "    df['GLD'] = scaler.fit_transform(df['GLD'].values.reshape(-1, 1))\n",
    "    # Fill in missing values with the mean of the previous and next values\n",
    "    df['GLD'] = df['GLD'].interpolate(method='linear')\n",
    "    return df\n",
    "\n",
    "# Chia dữ liệu thành train set và test set\n",
    "# @param  data                      Tập dữ liệu\n",
    "# @param  percentTrain              Tỷ lệ Tập train\n",
    "# @return train_data, test_data     Tập train và test   \n",
    "def splitData(data, percentTrain):\n",
    "    train_size = int(len(data) * (percentTrain/100))\n",
    "    train = data.iloc[:train_size, :]\n",
    "    test = data.iloc[train_size:, :]\n",
    "    return train, test\n",
    "\n",
    "# Xử lý dữ liệu thành dữ liệu đầu vào và đầu ra cho mô hình\n",
    "# @param      data            Dữ liệu cần chia cửa sổ\n",
    "# @param      size_window     Kích thước cửa sổ\n",
    "# @param      size_predict    Kích thước cửa sổ dự đoán\n",
    "# @param      stepWindow      số điểm dữ liệu trượt\n",
    "# @return     X, y            mảng cửa sổ mẫu và mảng điểm dự đoán tương ứng\n",
    "def prepare_data(data, size_window, size_predict, stepWindow):\n",
    "    X, y = [], []\n",
    "    startWindow = 0\n",
    "    for i in range(len(data) - size_window - 1):\n",
    "        if (len(data[(startWindow + size_window):(startWindow + size_window + size_predict) , 0]) != size_predict):\n",
    "            break\n",
    "        X.append(data[startWindow:(startWindow + size_window), :])\n",
    "        y.append(data[(startWindow + size_window):(startWindow + size_window + size_predict) , 0])\n",
    "        startWindow += stepWindow\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "#---------KNN-----------\n",
    "# Fucntion Tính khoảng cách giữa 2 chuỗi thời gian\n",
    "# @param    ts1            Chuỗi thứ nhất\n",
    "# @param    ts2            Chuỗi thứ hai\n",
    "# @return   euclidean      Khoảng cách euclidean    \n",
    "def euclidean_distance(ts1, ts2):\n",
    "    ts1= ts1.flatten()\n",
    "    ts2= ts2.flatten()\n",
    "    return euclidean(ts1,ts2)\n",
    "\n",
    "# Function lấy ra k chuỗi gần nhất\n",
    "# @param    k             Số lượng chuỗi gần nhất\n",
    "# @param    distanceArr   Mảng khoảng cách\n",
    "# @return   argsort       Vị trí chuỗi gần nhất        \n",
    "def kSimilarityTimeSeries(k, distanceArr):\n",
    "    distances = np.array(distanceArr)\n",
    "    return distances.argsort()[:k] \n",
    "\n",
    "# Tính khoảng cách DTW\n",
    "# @param    ts1            Chuỗi thứ nhất\n",
    "# @param    ts2            Chuỗi thứ hai\n",
    "# @return   euclidean      Khoảng cách euclidean  \n",
    "def dtw_dist(ts1, ts2):\n",
    "    dist, _, _, _ = dtw(ts1, ts2, dist=lambda ts1, ts2: np.abs(ts1 - ts2))\n",
    "    return dist\n",
    "\n",
    "# Thêm Dữ liệu\n",
    "# @param    X_train               Cửa sổ mẫu train\n",
    "# @param    y_train               Cửa sổ dự đoán train\n",
    "# @param    XTest                 Cửa sổ mẫu test\n",
    "# @param    yTest                 Cửa sổ dự đoán test\n",
    "# @return   X_train, y_train      Khoảng cách euclidean \n",
    "def toTrain(X_train, y_train, XTest, yTest):\n",
    "    X_train.append(XTest)\n",
    "    y_train.append(yTest)\n",
    "    return np.array(X_train), np.array(y_train)\n",
    "\n",
    "# Dự đoán Euclidean\n",
    "# @param    nameData           Tên tập dữ liệu\n",
    "# @param    k                  Số lượng chuỗi gần nhất\n",
    "# @param    typeDistance       Độ đo sử dụng (Dtw, euclidean)\n",
    "# @param    X_train            Cửa sổ mẫu train\n",
    "# @param    y_train            Cửa sổ dự đoán train\n",
    "# @param    X_test             Cửa sổ mẫu test\n",
    "# @param    y_test             Cửa sổ dự đoán test\n",
    "# @return   y_pred_arr         Mảng dự đoán\n",
    "def predict_KNN(k, typeDistance, X_train, y_train, X_test, y_test):\n",
    "    y_pred_arr=[]\n",
    "    for iTest in range(len(X_test)):\n",
    "        if(k>len(X_train)):\n",
    "            k=len(X_train)\n",
    "        distanceArr=[]\n",
    "        for iTrain in range(len(X_train)-size_window+2):\n",
    "            if(typeDistance == 'Dtw'):\n",
    "                distance = dtw_dist(X_test[iTest],X_train[iTrain])\n",
    "            else:\n",
    "                distance = euclidean_distance(X_test[iTest],X_train[iTrain])\n",
    "            distanceArr.append(distance)\n",
    "        indexKNN= kSimilarityTimeSeries(k,distanceArr)\n",
    "        y_pred = np.mean(y_train[indexKNN])\n",
    "        y_pred_arr.append(y_pred)\n",
    "        X_train, y_train = toTrain(X_train.tolist(), y_train.tolist(),X_test[iTest].tolist(), y_test[iTest].tolist())\n",
    "        y_pred = np.array(y_pred_arr)\n",
    "   \n",
    "    return y_pred\n",
    "\n",
    "\n",
    "#----------------FFNN--------------------------\n",
    "# Khởi tạo mô hình FFNN\n",
    "# @param    neuralInput         Kích thước Cửa sổ mẫu/ số neural lớp input\n",
    "# @param    num_layers_hidden   Số lượng lớp ẩn\n",
    "# @param    num_neural_hidden   Số neural lớp ẩn\n",
    "# @param    neuralOutput         Số neural lớp ouput\n",
    "# @return   model               Mô hình FFNN\n",
    "def create_model_FFNN(neuralInput, num_layers_hidden=1, neuralHidden=1, neuralOutput=1):\n",
    "    model = Sequential()\n",
    "    for i in range(num_layers_hidden):\n",
    "        if i == 0:\n",
    "            model.add(Dense(neuralHidden, input_dim= neuralInput, activation='sigmoid'))\n",
    "        else:\n",
    "            model.add(Dense(neuralHidden, activation='sigmoid'))\n",
    "    model.add(Dense(neuralOutput))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "\n",
    "# Train FFNN \n",
    "# @param    nameData          Tên tập dữ liệu\n",
    "# @param    size_window       Kích thước Cửa sổ mẫu/ số neural lớp input\n",
    "# @param    X_train           cửa sổ mẫu tập train\n",
    "# @param    y_train           Cửa sổ dự đoán tập train\n",
    "# @param    neuralHidden      Số neural lớp ẩn\n",
    "# @param    numHiddenLayer    Số lớp ẩn\n",
    "# @param    size_predict      Kích thước Cửa sổ dự đoán/ Số neural lớp ouput\n",
    "# @return   best_params_FFNN  Tham số tốt nhất cho mô hình FFNN                \n",
    "def train_FFNN(nameData, typePredict, size_window, X_train, y_train, neuralHidden, numHiddenLayer, size_predict):\n",
    "    #param search\n",
    "    param_grid_FFNN = {'batch_size': [8, 16, 32, 64, 128],\n",
    "              'epochs': [50, 100, 150, 200, 250, 300],\n",
    "              'neuralHidden': [neuralHidden],\n",
    "              'num_layers_hidden' : [numHiddenLayer],\n",
    "              'neuralInput' : [size_window],\n",
    "              'neuralOutput' : [size_predict]}\n",
    "\n",
    "    # create the model\n",
    "    model_FFNN = KerasRegressor(build_fn=create_model_FFNN, verbose=0)\n",
    "    \n",
    "    # perform the grid search\n",
    "    grid_FFNN = GridSearchCV(estimator=model_FFNN, param_grid=param_grid_FFNN, cv=3)\n",
    "    grid_result_FFNN = grid_FFNN.fit(X_train, y_train)\n",
    "    \n",
    "    # train the model with the best parameters\n",
    "    best_params_FFNN = grid_result_FFNN.best_params_\n",
    "    \n",
    "    model_FFNN = create_model_FFNN( best_params_FFNN['neuralInput'], best_params_FFNN['num_layers_hidden'], best_params_FFNN['neuralHidden'],best_params_FFNN['neuralOutput'])\n",
    "    model_FFNN.fit(X_train, y_train, epochs=best_params_FFNN['epochs'], batch_size=best_params_FFNN['batch_size'], verbose=2, callbacks=[EarlyStopping(monitor='loss', patience=10)], shuffle=False)\n",
    "    \n",
    "    if(typePredict=='FFNN_Find_NeuralHidden'):\n",
    "        model_FFNN.save_weights('../BestParam/FFNN/'+nameData+'/FFNN_Find_NeuralHidden/'+str(int(best_params_FFNN['num_layers_hidden']))+'_HiddenLayer_'+str(int(best_params_FFNN['neuralHidden']))+'_NeuralHidden_'+str(int(best_params_FFNN['batch_size']))+'_BatchSize_'+str(int(best_params_FFNN['epochs']))+'_Epoch_'+nameData+'.h5')   \n",
    "    elif (typePredict=='FFNN_Find_NumberHiddenLayer'):\n",
    "        model_FFNN.save_weights('../BestParam/FFNN/'+nameData+'/FFNN_Find_NumberHiddenLayer/'+str(int(best_params_FFNN['num_layers_hidden']))+'_HiddenLayer_'+str(int(best_params_FFNN['neuralHidden']))+'_NeuralHidden_'+str(int(best_params_FFNN['batch_size']))+'_BatchSize_'+str(int(best_params_FFNN['epochs']))+'_Epoch_'+nameData+'.h5')   \n",
    "    else:\n",
    "        model_FFNN.save_weights('../BestParam/TuanTu/'+nameData+'/FFNN_Find_NumberHiddenLayer/'+str(int(best_params_FFNN['num_layers_hidden']))+'_HiddenLayer_'+str(int(best_params_FFNN['neuralHidden']))+'_NeuralHidden_'+str(int(best_params_FFNN['batch_size']))+'_BatchSize_'+str(int(best_params_FFNN['epochs']))+'_Epoch_'+nameData+'.h5')   \n",
    "    return best_params_FFNN\n",
    "\n",
    "\n",
    "\n",
    "# Train FFNN \n",
    "# @param    nameData          Tên tập dữ liệu\n",
    "# @param    size_window       Kích thước Cửa sổ mẫu/ số neural lớp input\n",
    "# @param    X_train           cửa sổ mẫu tập train\n",
    "# @param    y_train           Cửa sổ dự đoán tập train\n",
    "# @param    batchSize         Số lượng mẫu được đưa vào với mỗi lần lặp (epoch)\n",
    "# @param    epoch             Số lần lặp cập nhật trọng số\n",
    "# @param    neuralHidden      Số neural lớp ẩn\n",
    "# @param    numHiddenLayer    Số lớp ẩn\n",
    "# @param    size_predict      Kích thước Cửa sổ dự đoán/ Số neural lớp ouput\n",
    "# @return   best_params_FFNN  Tham số tốt nhất cho mô hình FFNN                \n",
    "def train_best_param_FFNN(nameData, typePredict, size_window, X_train, y_train, batchSize, epoch, neuralHidden, numHiddenLayer, size_predict):\n",
    "    #param search\n",
    "    param_grid_FFNN = {'batch_size': batchSize,\n",
    "              'epochs': epoch,\n",
    "              'neuralHidden': neuralHidden,\n",
    "              'num_layers_hidden' : numHiddenLayer,\n",
    "              'neuralInput' : size_window,\n",
    "              'neuralOutput' : size_predict}\n",
    "    \n",
    "    model_FFNN = create_model_FFNN( param_grid_FFNN['neuralInput'], param_grid_FFNN['num_layers_hidden'], param_grid_FFNN['neuralHidden'],param_grid_FFNN['neuralOutput'])\n",
    "    model_FFNN.fit(X_train, y_train, epochs=param_grid_FFNN['epochs'], batch_size=param_grid_FFNN['batch_size'], verbose=2, callbacks=[EarlyStopping(monitor='loss', patience=10)], shuffle=False)\n",
    "    \n",
    "    if(typePredict=='FFNN_Find_NeuralHidden'):\n",
    "        model_FFNN.save_weights('../BestParam/FFNN/'+nameData+'/FFNN_Find_NeuralHidden/'+str(int(param_grid_FFNN['num_layers_hidden']))+'_HiddenLayer_'+str(int(param_grid_FFNN['neuralHidden']))+'_NeuralHidden_'+str(int(param_grid_FFNN['batch_size']))+'_BatchSize_'+str(int(param_grid_FFNN['epochs']))+'_Epoch_'+nameData+'.h5')   \n",
    "    elif (typePredict=='FFNN_Find_NumberHiddenLayer'):\n",
    "        model_FFNN.save_weights('../BestParam/FFNN/'+nameData+'/FFNN_Find_NumberHiddenLayer/'+str(int(param_grid_FFNN['num_layers_hidden']))+'_HiddenLayer_'+str(int(param_grid_FFNN['neuralHidden']))+'_NeuralHidden_'+str(int(param_grid_FFNN['batch_size']))+'_BatchSize_'+str(int(param_grid_FFNN['epochs']))+'_Epoch_'+nameData+'.h5')   \n",
    "    elif (typePredict=='FFNN_Find_NumberHiddenLayer_SongSong'):\n",
    "        model_FFNN.save_weights('../BestParam/SongSong/'+nameData+'/FFNN_Find_BestWeights/'+str(int(param_grid_FFNN['num_layers_hidden']))+'_HiddenLayer_'+str(int(param_grid_FFNN['neuralHidden']))+'_NeuralHidden_'+str(int(param_grid_FFNN['batch_size']))+'_BatchSize_'+str(int(param_grid_FFNN['epochs']))+'_Epoch_'+nameData+'.h5')   \n",
    "    else:\n",
    "        model_FFNN.save_weights('../BestParam/TuanTu/'+nameData+'/FFNN_Find_BestWeights/'+str(int(param_grid_FFNN['num_layers_hidden']))+'_HiddenLayer_'+str(int(param_grid_FFNN['neuralHidden']))+'_NeuralHidden_'+str(int(param_grid_FFNN['batch_size']))+'_BatchSize_'+str(int(param_grid_FFNN['epochs']))+'_Epoch_'+nameData+'.h5')   \n",
    "   \n",
    "    return param_grid_FFNN\n",
    "\n",
    "# Dự Đoán FFNN \n",
    "# @param    nameData          Tên tập dữ liệu\n",
    "# @param    typePredict       Thực hiện loại dự đoán (FFNN_Find_NeuralHidden,FFNN_Find_NumberHiddenLayer,CombinePredict)\n",
    "# @param    X_train           cửa sổ mẫu tập train\n",
    "# @param    y_train           Cửa sổ dự đoán tập train\n",
    "# @param    X_test            Cửa sổ dự đoán tập train\n",
    "# @param    best_params_FFNN  Cửa sổ dự đoán tập train\n",
    "# @return   predictions_FFNN  Mảng dự đoán\n",
    "def predict_FFNN(nameData, typePredict, X_train, y_train, X_test, best_params_FFNN):\n",
    "    model_FFNN1 = Sequential()\n",
    "    for i in range(best_params_FFNN['num_layers_hidden']):\n",
    "        if i == 0:\n",
    "            model_FFNN1.add(Dense(best_params_FFNN['neuralHidden'], input_dim= best_params_FFNN['neuralInput'], activation='sigmoid'))\n",
    "        else:\n",
    "            model_FFNN1.add(Dense(best_params_FFNN['neuralHidden'], activation='sigmoid'))\n",
    "    model_FFNN1.add(Dense(best_params_FFNN['neuralOutput']))\n",
    "    \n",
    "    if(typePredict=='FFNN_Find_NeuralHidden'):\n",
    "        model_FFNN1.load_weights('../BestParam/FFNN/'+nameData+'/FFNN_Find_NeuralHidden/'+str(int(best_params_FFNN['num_layers_hidden']))+'_HiddenLayer_'+str(int(best_params_FFNN['neuralHidden']))+'_NeuralHidden_'+str(int(best_params_FFNN['batch_size']))+'_BatchSize_'+str(int(best_params_FFNN['epochs']))+'_Epoch_'+nameData+'.h5')   \n",
    "    elif (typePredict=='FFNN_Find_NumberHiddenLayer'):\n",
    "        model_FFNN1.load_weights('../BestParam/FFNN/'+nameData+'/FFNN_Find_NumberHiddenLayer/'+str(int(best_params_FFNN['num_layers_hidden']))+'_HiddenLayer_'+str(int(best_params_FFNN['neuralHidden']))+'_NeuralHidden_'+str(int(best_params_FFNN['batch_size']))+'_BatchSize_'+str(int(best_params_FFNN['epochs']))+'_Epoch_'+nameData+'.h5')   \n",
    "    elif (typePredict=='FFNN_Find_NumberHiddenLayer_SongSong'):\n",
    "        model_FFNN1.load_weights('../BestParam/SongSong/'+nameData+'/FFNN_Find_BestWeights/'+str(int(best_params_FFNN['num_layers_hidden']))+'_HiddenLayer_'+str(int(best_params_FFNN['neuralHidden']))+'_NeuralHidden_'+str(int(best_params_FFNN['batch_size']))+'_BatchSize_'+str(int(best_params_FFNN['epochs']))+'_Epoch_'+nameData+'.h5')   \n",
    "    else:\n",
    "        model_FFNN1.load_weights('../BestParam/TuanTu/'+nameData+'/FFNN_Find_BestWeights/'+str(int(best_params_FFNN['num_layers_hidden']))+'_HiddenLayer_'+str(int(best_params_FFNN['neuralHidden']))+'_NeuralHidden_'+str(int(best_params_FFNN['batch_size']))+'_BatchSize_'+str(int(best_params_FFNN['epochs']))+'_Epoch_'+nameData+'.h5')   \n",
    "   \n",
    "    model_FFNN1.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    \n",
    "    predictions_FFNN = model_FFNN1.predict(X_test)\n",
    "    return predictions_FFNN\n",
    "\n",
    "#---------------------Song Song------------------------------#\n",
    "\n",
    "# Lai Ghép Song Song\n",
    "# @param    y_pred_FFNN     Mảng dự đoán của FFNN\n",
    "# @param    y_pred_KNN      Mảng dự đoán của KNN\n",
    "# @param    y_test          Mảng chuỗi thực tế\n",
    "# @return   y_pred_combine  Mảng dự đoán kết hợp\n",
    "def predictHybrid(y_pred_FFNN,y_pred_KNN,y_test):\n",
    "    FFNNSubKNN=[]\n",
    "    TestSubKNN=[]\n",
    "    weightEl=[]\n",
    "    for i in range(len(y_pred_FFNN)):\n",
    "        FFNNSubKNN.append(y_pred_FFNN[i]-y_pred_KNN[i])\n",
    "        TestSubKNN.append(y_test[i]-y_pred_KNN[i])\n",
    "\n",
    "    for j in range(len(FFNNSubKNN)):\n",
    "        weightEl.append(((FFNNSubKNN[j]*TestSubKNN[j])/ (FFNNSubKNN[j]*FFNNSubKNN[j])))\n",
    "    \n",
    "    weight = np.array(weightEl)\n",
    "    \n",
    "    y_pred_combine=[]\n",
    "    for i in range(len(weight)):\n",
    "        y_pred_combine.append(weight[i]*y_pred_FFNN[i]+(1-weight[i])*y_pred_KNN[i])\n",
    "    y_pred_combine=np.array(y_pred_combine)\n",
    "    return y_pred_combine\n",
    "\n",
    "#---------------------Tuần Tự------------------------------#\n",
    "\n",
    "# Tính Lỗi Theo Từng Ngày\n",
    "# @param    y_pred_KNN      Mảng dự đoán của KNN\n",
    "# @param    y_test          Mảng chuỗi thực tế\n",
    "# @return   mseWithDay     Mảng lỗi theo ngày\n",
    "def mseWithDay(y_pred_KNN, y_test):\n",
    "    mseDay = []\n",
    "    for i in range(len(y_pred_KNN)):\n",
    "        mseDay.append([abs((y_test[i] - y_pred_KNN[i]))])\n",
    "    mseDay=np.array(mseDay)\n",
    "    return mseDay\n",
    "\n",
    "def predictSum(y_pred_mse,y_pred_KNN):\n",
    "    pred_knn_ffnn=[]\n",
    "    for i in range(len(y_pred_mse)):\n",
    "        pred_knn_ffnn.append(y_pred_KNN[i]+y_pred_mse[i])\n",
    "    pred_knn_ffnn=np.array(pred_knn_ffnn)\n",
    "    return pred_knn_ffnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/GLD.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m percentTrain \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m80\u001b[39m\n\u001b[0;32m      6\u001b[0m filePath\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39mnameData\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 7\u001b[0m timeSeries \u001b[38;5;241m=\u001b[39m \u001b[43mreadData\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilePath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m timeSeries \u001b[38;5;241m=\u001b[39m cleanData(timeSeries)\n\u001b[0;32m      9\u001b[0m train_data, test_data \u001b[38;5;241m=\u001b[39m splitData(timeSeries, percentTrain)\n",
      "Cell \u001b[1;32mIn[4], line 6\u001b[0m, in \u001b[0;36mreadData\u001b[1;34m(filePath)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreadData\u001b[39m(filePath):\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# Load dữ liệu\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m     dataCSV \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilePath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m     df\u001b[38;5;241m=\u001b[39mdataCSV[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGLD\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Python312\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/GLD.csv'"
     ]
    }
   ],
   "source": [
    "df_MseWith_K_Euclidean = pd.DataFrame([],  columns =  [\"K\", \"MSE\",\"Time\"])\n",
    "df_MseWith_K_Dtw = pd.DataFrame([],  columns =  [\"K\", \"MSE\",\"Time\"])\n",
    "\n",
    "nameData= 'GLD'\n",
    "percentTrain = 80\n",
    "filePath= './data/'+nameData+'.csv'\n",
    "timeSeries = readData(filePath)\n",
    "timeSeries = cleanData(timeSeries)\n",
    "train_data, test_data = splitData(timeSeries, percentTrain)\n",
    "for k in range(1,21):\n",
    "    #Eculidean\n",
    "    size_window = 7\n",
    "    size_predict=1\n",
    "    stepWindow=1\n",
    "    \n",
    "    X_train, y_train = prepare_data(train_data.values, size_window, size_predict, stepWindow)\n",
    "    X_test, y_test =   prepare_data(test_data.values, size_window, size_predict, stepWindow)\n",
    "    \n",
    "    start_KNN_Euclidean= time.time()\n",
    "    y_pred_KNN_Euclidean= predict_KNN(k, 'Euclidean', X_train, y_train, X_test, y_test)\n",
    "    end_KNN_Euclidean = time.time()\n",
    "    np.savetxt('BestParam/KNN/'+nameData+'/KNN_Euclidean_Find_K/'+str(k)+'_K_'+nameData+'.txt', y_pred_KNN_Euclidean)    \n",
    "    rowMseWithK_Euclidean = pd.Series([k,mean_squared_error(y_test , y_pred_KNN_Euclidean), (end_KNN_Euclidean - start_KNN_Euclidean)], index=df_MseWith_K_Euclidean.columns)\n",
    "    df_MseWith_K_Euclidean = df_MseWith_K_Euclidean.append(rowMseWithK_Euclidean,ignore_index=True)\n",
    "\n",
    "    #Dtw\n",
    "    X_train, y_train = prepare_data(train_data.values, size_window, size_predict, stepWindow)\n",
    "    X_test, y_test = prepare_data(test_data.values, size_window, size_predict, stepWindow)\n",
    "    start_KNN_Dtw= time.time()\n",
    "    y_pred_KNN_Dtw = predict_KNN(k, 'Dtw', X_train, y_train, X_test, y_test)\n",
    "    end_KNN_Dtw= time.time()\n",
    "    np.savetxt('BestParam/KNN/'+nameData+'/KNN_Dtw_Find_K/'+str(k)+'_K_'+nameData+'.txt', y_pred_KNN_Dtw)    \n",
    "    rowMseWithK_Dtw = pd.Series([k, mean_squared_error(y_test , y_pred_KNN_Dtw), (end_KNN_Dtw - start_KNN_Dtw)], index=df_MseWith_K_Dtw.columns)\n",
    "    df_MseWith_K_Dtw = df_MseWith_K_Dtw.append(rowMseWithK_Dtw,ignore_index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
